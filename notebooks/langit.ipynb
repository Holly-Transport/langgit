{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab04dac3",
   "metadata": {},
   "source": [
    "# Langchain + Github = Langgit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80325b6",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries & Set Up API Keys\n",
    "Make sure you already installed the requirments (pip install -r requirements.txt) and added your [OpenAI Key](https://platform.openai.com/docs/api-reference) (export OPENAI_API_KEY = xxxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d835b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIBRARIES! ##\n",
    "\n",
    "# Import Libraries for API Useage\n",
    "import os\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "# Import Langchain Libraries\n",
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "\n",
    "# Import Libraries to Pull URL Lists\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Import Library to Make Results Pretty\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5152702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPENAI KEY ##\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eee3c5",
   "metadata": {},
   "source": [
    "### Step 2: Select a WB Data Lab Data Good URL\n",
    "This notebook has been designed to support searching Data Goods produced by the WB Data Lab -- GitHub-generated Jupyter books. That said, the notebook should work with other urls, too. Please note, though, that this notebook hasn't been optimized in any way, so more complex websites may require additional effort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "820a902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://datapartnership.org/syria-economic-monitor/README.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55eeb24",
   "metadata": {},
   "source": [
    "### Step 3: Using Beautiful Soup, Generate a List of Sub-URLs\n",
    "Beautiful Soup is a commonly used Python library for scraping web sites. This step takes a domain and follows links to identify other sub-domain websites. Uncomment the \"Test: Print URLs\" to make sure it works the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6f56071",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "urls = []\n",
    "for link in soup.find_all('a'):\n",
    "    href = link.get('href')\n",
    "    if href and not href.startswith('#'):  # Exclude anchor links\n",
    "        absolute_url = urljoin(url, href)\n",
    "        urls.append(absolute_url)\n",
    "\n",
    "## TEST: PRINT URLS ##\n",
    "# print (urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5dc63d",
   "metadata": {},
   "source": [
    "### Step 4: Read in Contents from All Listed URLs \n",
    "Langchain is a Language Learning Model library that is commonly used to support interactions between generative AI and existing datasets -- text, .csv files, .pdf files, .json's, and so many more. In this step, we use the Selenium URL loader to read in the contents of each of the URLs retreived in the previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6483a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ IN URL CONTENTS FOR USE WITH LANGCHAIN ##\n",
    "\n",
    "loader = SeleniumURLLoader(urls=urls)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2fa5e",
   "metadata": {},
   "source": [
    "### Step 5: Break Contents into \"Chunks\"\n",
    "In this step, we use a Langchain Text Splitter (CharacterTextSplitter) to break the website content into encoded chunks, or tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b1f5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPLIT URL CONTENTS INTO CHUNKS ##\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator = \"\\n\\n\", chunk_size=2500, chunk_overlap=200, length_function=len,)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a3487",
   "metadata": {},
   "source": [
    "### Step 6: Vectorize the Data\n",
    "In this step, we use OpenAI Embeddings and FAISS Vectorstore Tool to make searching our data more efficient. The easiest way to think about this step is to imagine we have a .csv file. Each vector would be one row of the .csv. Then, in a later step, if we make a query that includes a reference in one of the rows, only that row of information (vector) would then be passed on to the OpenAI API for a natural language response. This prevents us from having to send entire datasets through the API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7f0c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VECTORIZE THE CHUNKS FOR EFFICIENT SEARCH ##\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a56a7e5",
   "metadata": {},
   "source": [
    "### Step 7: Save Vector Store\n",
    "Now, we will save the vectorized data (Vector Store), so we can go back to it anytime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab96ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE EMBEDDINGS LOCALLY, SO YOU DO NOT HAVE TO REGENRATE ALL THE TIME ##\n",
    "\n",
    "db.save_local(\"faiss_index\")\n",
    "new_db = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44a3833",
   "metadata": {},
   "source": [
    "### Step 8: Set Up and Test Retreiver\n",
    "The Retreiver will search the vector store to idenitfy those vectors that have the most relevant information based on the user query. Only these vectors will be retreived and sent to the ChatGPT API for processing. The code below includes a couple tests that are recommended the first time you run this notebook, to make sure the embeddings (vector process) and retreiver work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cde7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST EMBEDDINGS ##\n",
    "\n",
    "# query = \"Which governorate experienced the most tree cover loss?\"\n",
    "# docs = new_db.similarity_search(query)\n",
    "# print(docs)\n",
    "\n",
    "## SET UP AND TEST RETREIVER ##\n",
    "retriever = new_db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .7})\n",
    "# docs = retriever.get_relevant_documents(\"Which governorate had the biggest change in tree cover?\")\n",
    "# print (docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674b8a5",
   "metadata": {},
   "source": [
    "### Step 9: Set Up Query Chain \n",
    "Now, using Langchain RetrievalQAWithSourcesChain, ChatOpenAI (it's good!), and your Retreiver, we are ready to go! The following code takes a query, finds the relevant data in your vector store, and then sends that data as part of a prompt to the ChatGPT API. If you find too much information is being sent through (exceeding the OpenAI token limits), you may consider increasing the similarity score threshold used by the retreiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e96bc908",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET UP QUERY CHAIN. FIRST RETREIVE VECTORS FROM DATABASE. THEN SEND TO OPENAI FOR GENERATING RESPONSE ##\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(ChatOpenAI(temperature=0), \n",
    "                                                    chain_type=\"stuff\", \n",
    "                                                    retriever=new_db.as_retriever(search_type=\"similarity_score_threshold\", \n",
    "                                                                                  search_kwargs={\"score_threshold\": .7}))\n",
    "\n",
    "\n",
    "## THIS METHOD USES OPENAI, INSTEAD OF CHATOPENAI -- CHATOPENAI IS SO MUCH BETTER!!! ##\n",
    "# chain = RetrievalQAWithSourcesChain.from_chain_type(OpenAI(temperature=0), \n",
    "#                                                     chain_type=\"stuff\", \n",
    "#                                                     retriever=new_db.as_retriever(search_type=\"similarity_score_threshold\", \n",
    "#                                                                                   search_kwargs={\"score_threshold\": .7}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd85890b",
   "metadata": {},
   "source": [
    "### Step 10: Big Time! Ask Your Content a Question!\n",
    "Now we can write a question for our Data Good (or other fed-in url content!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2aa4d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " How were humanitarian surveys used in the Syria Economic Monitor?\n"
     ]
    }
   ],
   "source": [
    "## ASK A QUESTION! ##\n",
    "question = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65bbb0",
   "metadata": {},
   "source": [
    "### Step 11: Run a Query and Make the Response Pretty\n",
    "Warning! Even with all this splitting, vectorizing, and retreiving, sometimes we still exceed the paltry ChatGPT token limits. To solve, make your question more specific to reduce the number of retreived materials, or increase the \"score_treshold\" in your query chain. In the response, below, note that we also include the sources -- important and exciting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d7f7281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question:**\n",
       "\n",
       "How were humanitarian surveys used in the Syria Economic Monitor?\n",
       "\n",
       "**Answer:**\n",
       "\n",
       "Humanitarian surveys were used in the Syria Economic Monitor to collect data on community situations and needs relating to shelter, electricity, water sanitation and hygiene (WASH), food security, livelihoods, health, education, humanitarian assistance, and priority needs. The survey used key informant interviews to collect data at the community (admin4) level. The panel used for the analysis includes 1,426 communities (371 in NWS and 1246 in NES) which are included in all rounds of data collection. The data and methodology used to generate insights for this project have been prepared as Data Goods, which are designed to be re-used for future updates and projects. Nighttime lights were also analyzed to understand the economic impacts of the February 6 earthquake in Turkiye and northern Syria. \n",
       "\n",
       "\n",
       "**Sources:**\n",
       "\n",
       "https://github.com/datapartnership/syria-economic-monitor, https://datapartnership.org/syria-economic-monitor/notebooks/hsos-survey/hsos-survey-readme.html, https://datapartnership.org/syria-economic-monitor/docs/2023-summer-economic-monitor.html\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## RUN QUERY AND MAKE RESPONSE PRETTY ##\n",
    "\n",
    "response = chain({\"question\": question}, return_only_outputs=True)\n",
    "markdown_text = f\"**Question:**\\n\\n{question}\\n\\n**Answer:**\\n\\n{response['answer']}\\n\\n**Sources:**\\n\\n{response['sources']}\\n\\n\"\n",
    "display(Markdown(markdown_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f756600-77b9-485f-9cb4-1442a5cb69fa",
   "metadata": {},
   "source": [
    "### Final Notes\n",
    "I am a novice! I have never written an application like this before! My explanations and code are based on what I have taught myself in the past couple months, so if you see areas for improvement, or, more importantly, things that are just completely wrong, please submit an issue on this repo, so I can learn and fix <3\n",
    "\n",
    "After preparing this and other notebooks, I have come to see generative AI as a Matrix-style superpower, enabling me to do much more, much faster, than I could have ever dreamed of. Maybe this is dangerous! Maybe this is the first step to reaching the next galaxy (because to do that, we can't just so what we are doing now...). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
