{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries & Set Up API Keys\n",
    "Make sure you already installed the requirments (pip install -r requirements.txt) and added your [OpenAI Key](https://platform.openai.com/docs/api-reference) (export OPENAI_KEY = xxxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIBRARIES! ##\n",
    "\n",
    "# Import Libraries for API Useage\n",
    "import os\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "# Import Langchain Libraries\n",
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "\n",
    "# Import Libraries to Pull URL Lists\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Import Library to Make Results Pretty\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPENAI KEY ##\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Select a WB Data Lab Data Good URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://datapartnership.org/syria-economic-monitor/README.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Using Beautiful Soup, Generate a List of Sub-URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "urls = []\n",
    "for link in soup.find_all('a'):\n",
    "    href = link.get('href')\n",
    "    if href and not href.startswith('#'):  # Exclude anchor links\n",
    "        absolute_url = urljoin(url, href)\n",
    "        urls.append(absolute_url)\n",
    "\n",
    "## TEST: PRINT URLS ##\n",
    "# print (urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Using a Langchain URL Loader, Read in Contents from All Listed URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ IN URL CONTENTS FOR USE WITH LANGCHAIN ##\n",
    "\n",
    "loader = SeleniumURLLoader(urls=urls)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Use a Langchain Text Splitter (CharacterTextSplitter) to Break Contents into \"Chunks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPLIT URL CONTENTS INTO CHUNKS ##\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator = \"\\n\\n\", chunk_size=2500, chunk_overlap=200, length_function=len,)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Use OpenAI Embeddings and FAISS Vectorstore Tool to, um, Vectorize the Data to Make Search More Efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VECTORIZE THE CHUNKS FOR EFFICIENT SEARCH ##\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Save Vectore Store, So You Can Go Back to it Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE EMBEDDINGS LOCALLY, SO YOU DO NOT HAVE TO REGENRATE ALL THE TIME ##\n",
    "\n",
    "db.save_local(\"faiss_index\")\n",
    "new_db = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Test Embeddings and Set Up and Test Retreiver\n",
    "The Retreiver will use the vectore store to idenitfy those, um, vectors that have the most relevant information based on the user query. Only these vectors will be sent to the ChatGPT API for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST EMBEDDINGS ##\n",
    "\n",
    "# query = \"What is GDP of Kenya in 2023?\"\n",
    "# docs = new_db.similarity_search(query)\n",
    "# print(docs)\n",
    "\n",
    "## SET UP AND TEST RETREIVER ##\n",
    "retriever = new_db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .7})\n",
    "# docs = retriever.get_relevant_documents(\"Which governorate had the biggest change in tree cover?\")\n",
    "# print (docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Set Up Query Chain using Langchain RetrievalQAWithSourcesChain, ChatOpenAI (it's good!), and Your Retreiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET UP QUERY CHAIN. FIRST RETREIVE VECTORS FROM DATABASE. THEN SEND TO OPENAI FOR GENERATING RESPONSE ##\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(ChatOpenAI(temperature=0), \n",
    "                                                    chain_type=\"stuff\", \n",
    "                                                    retriever=new_db.as_retriever(search_type=\"similarity_score_threshold\", \n",
    "                                                                                  search_kwargs={\"score_threshold\": .7}))\n",
    "\n",
    "\n",
    "## THIS METHOD USES OPENAI, INSTEAD OF CHATOPENAI -- CHATOPENAI IS SO MUCH BETTER!!! ##\n",
    "# chain = RetrievalQAWithSourcesChain.from_chain_type(OpenAI(temperature=0), \n",
    "#                                                     chain_type=\"stuff\", \n",
    "#                                                     retriever=new_db.as_retriever(search_type=\"similarity_score_threshold\", \n",
    "#                                                                                   search_kwargs={\"score_threshold\": .7}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Big Time! Ask Your Data Good a Question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " How were humanitarian surveys used in the Syria Economic Monitor?\n"
     ]
    }
   ],
   "source": [
    "## ASK A QUESTION! ##\n",
    "question = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Run a Query and Make the Response Pretty\n",
    "Warning! Even with all this splitting, vectorizing, and retreiving, sometimes we still exceed the paltry ChatGPT token limits. To solve, make your question more specific to reduce the number of retreived materials, or increase the \"score_treshold\" in your query chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1b667d6c9985cd71fccacf01612105b0 in your message.).\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question:**\n",
       "\n",
       "How were humanitarian surveys used in the Syria Economic Monitor?\n",
       "\n",
       "**Answer:**\n",
       "\n",
       "Humanitarian surveys were used in the Syria Economic Monitor to collect data on community situations and needs relating to shelter, electricity, water sanitation and hygiene (WASH), food security, livelihoods, health, education, humanitarian assistance, and priority needs. The survey used key informant interviews to collect data at the community (admin4) level. The panel used for the analysis includes 1,426 communities (371 in NWS and 1246 in NES) which are included in all rounds of data collection. The data and methodology used to generate insights for this project have been prepared as Data Goods, which are designed to be re-used for future updates and projects. Nighttime lights were also analyzed to understand the economic impacts of the February 6 earthquake in Turkiye and northern Syria. \n",
       "\n",
       "\n",
       "**Sources:**\n",
       "\n",
       "https://github.com/datapartnership/syria-economic-monitor, https://datapartnership.org/syria-economic-monitor/notebooks/hsos-survey/hsos-survey-readme.html, https://datapartnership.org/syria-economic-monitor/docs/2023-summer-economic-monitor.html\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## RUN QUERY AND MAKE RESPONSE PRETTY ##\n",
    "\n",
    "response = chain({\"question\": question}, return_only_outputs=True)\n",
    "markdown_text = f\"**Question:**\\n\\n{question}\\n\\n**Answer:**\\n\\n{response['answer']}\\n\\n**Sources:**\\n\\n{response['sources']}\\n\\n\"\n",
    "display(Markdown(markdown_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
